{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20904637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 mel-spectrogram files\n",
      "Found 25 snap parameter files\n",
      "Loaded 0 samples\n",
      "No valid data found!\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn , optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load preprocessed data\n",
    "preprocessed_dir = Path(\"preprocessed\")\n",
    "snaps_dir = Path(\"preprocessed_snaps\")\n",
    "\n",
    "# Load mel-spectrograms (.npy files)\n",
    "mel_files = list(preprocessed_dir.glob(\"*.npy\"))\n",
    "print(f\"Found {len(mel_files)} mel-spectrogram files\")\n",
    "\n",
    "# Load corresponding snap parameters (.json files)\n",
    "snap_files = list(snaps_dir.glob(\"*.json\"))\n",
    "print(f\"Found {len(snap_files)} snap parameter files\")\n",
    "\n",
    "# Create X (mel-spectrograms) and y (snap parameters)\n",
    "X_data = []\n",
    "y_data = []\n",
    "\n",
    "def extract_numeric_values(data):\n",
    "    \"\"\"Extract only numeric values from snap parameters\"\"\"\n",
    "    numeric_values = []\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if isinstance(value, (int, float)):\n",
    "                numeric_values.append(float(value))\n",
    "            elif isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if isinstance(item, (int, float)):\n",
    "                        numeric_values.append(float(item))\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            if isinstance(item, (int, float)):\n",
    "                numeric_values.append(float(item))\n",
    "    \n",
    "    return numeric_values\n",
    "\n",
    "for mel_file in mel_files:\n",
    "    # Load mel-spectrogram\n",
    "    mel = np.load(mel_file)\n",
    "    \n",
    "    # Find corresponding snap file\n",
    "    snap_file = snaps_dir / (mel_file.stem + \".json\")\n",
    "    if snap_file.exists():\n",
    "        with open(snap_file, 'r') as f:\n",
    "            snap_params = json.load(f)\n",
    "        \n",
    "        # Extract only numeric values\n",
    "        numeric_values = extract_numeric_values(snap_params)\n",
    "        \n",
    "        if len(numeric_values) > 0:  # Only add if we have numeric values\n",
    "            X_data.append(torch.tensor(mel, dtype=torch.float32))\n",
    "            y_data.append(torch.tensor(numeric_values, dtype=torch.float32))\n",
    "\n",
    "print(f\"Loaded {len(X_data)} samples\")\n",
    "if len(X_data) > 0:\n",
    "    print(f\"Mel shape: {X_data[0].shape}\")\n",
    "    print(f\"Snap shape: {y_data[0].shape}\")\n",
    "    print(f\"Snap values example: {y_data[0][:5]}\")  # Show first 5 values\n",
    "\n",
    "# Convert to tensors\n",
    "if len(X_data) > 0:\n",
    "    X = torch.stack(X_data)  # Shape: (N, mel_bins, time_frames)\n",
    "    y = torch.stack(y_data)   # Shape: (N, snap_params)\n",
    "    \n",
    "    # Transpose X to (N, time_frames, mel_bins) for RNN\n",
    "    X = X.transpose(1, 2)\n",
    "    N, L, Hin = X.shape\n",
    "    print(f\"Final X shape: {X.shape}, y shape: {y.shape}\")\n",
    "else:\n",
    "    print(\"No valid data found!\")\n",
    "    X = torch.randn(4, 3, 2)  # Fallback dummy data\n",
    "    y = torch.randn(4, 5)      # Fallback dummy targets\n",
    "    N, L, Hin = X.shape\n",
    "\n",
    "batchsize = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31beedf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: torch.Size([3, 3, 2]), Test: torch.Size([1, 3, 2])\n",
      "Batch 0: X shape: torch.Size([2, 3, 2]), y shape: torch.Size([2, 5])\n",
      "Batch 1: X shape: torch.Size([1, 3, 2]), y shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create train/test split\n",
    "from torch._tensor import Tensor\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader[tuple[Tensor, ...]](train_dataset, batch_size=batchsize, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batchsize, shuffle=False)\n",
    "\n",
    "# Test data loading\n",
    "for count, (xb, yb) in enumerate(train_loader):\n",
    "    print(f\"Batch {count}: X shape: {xb.shape}, y shape: {yb.shape}\")\n",
    "    if count >= 2:  # Show first 3 batches\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ef6eec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input features (mel bins): 2\n",
      "Output features (snap params): 5\n",
      "Hidden size: 64\n"
     ]
    }
   ],
   "source": [
    "# Model Architecture\n",
    "Hout = 64  # Hidden state size (increased for better capacity)\n",
    "output_dim = y.shape[1]  # Number of snap parameters to predict\n",
    "\n",
    "print(f\"Input features (mel bins): {Hin}\")\n",
    "print(f\"Output features (snap params): {output_dim}\")\n",
    "print(f\"Hidden size: {Hout}\")\n",
    "\n",
    "# Initialize weights\n",
    "Wx = torch.randn(Hout, Hin) * 0.1\n",
    "bx = torch.zeros(Hout)\n",
    "Wh = torch.randn(Hout, Hout) * 0.1\n",
    "bh = torch.zeros(Hout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ba8917f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN backbone created successfully\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create RNN backbone\n",
    "backbone = nn.RNN(input_size=Hin, hidden_size=Hout, batch_first=True)\n",
    "backbone.weight_ih_l0.data = Wx\n",
    "backbone.bias_ih_l0.data = bx\n",
    "backbone.weight_hh_l0.data = Wh\n",
    "backbone.bias_hh_l0.data = bh\n",
    "\n",
    "print(\"RNN backbone created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea4e511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Head layer: 64 -> 5\n",
      "Model created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create prediction head\n",
    "head = nn.Linear(in_features=Hout, out_features=output_dim)\n",
    "print(f\"Head layer: {Hout} -> {output_dim}\")\n",
    "\n",
    "# Combine model components\n",
    "model = nn.ModuleList([backbone, head])\n",
    "print(\"Model created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96e0daf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:00<00:00, 559.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 0.8480\n",
      "Epoch 2, Average Loss: 0.8920\n",
      "Epoch 3, Average Loss: 0.8128\n",
      "Epoch 4, Average Loss: 0.7216\n",
      "Epoch 5, Average Loss: 0.7231\n",
      "Epoch 6, Average Loss: 0.7231\n",
      "Epoch 7, Average Loss: 0.6210\n",
      "Epoch 8, Average Loss: 0.5940\n",
      "Epoch 9, Average Loss: 0.5513\n",
      "Epoch 10, Average Loss: 0.5276\n",
      "Epoch 11, Average Loss: 0.4663\n",
      "Epoch 12, Average Loss: 0.4687\n",
      "Epoch 13, Average Loss: 0.4816\n",
      "Epoch 14, Average Loss: 0.4533\n",
      "Epoch 15, Average Loss: 0.3972\n",
      "Epoch 16, Average Loss: 0.4024\n",
      "Epoch 17, Average Loss: 0.3638\n",
      "Epoch 18, Average Loss: 0.3628\n",
      "Epoch 19, Average Loss: 0.2524\n",
      "Epoch 20, Average Loss: 0.3301\n",
      "Epoch 21, Average Loss: 0.3141\n",
      "Epoch 22, Average Loss: 0.3228\n",
      "Epoch 23, Average Loss: 0.2863\n",
      "Epoch 24, Average Loss: 0.3142\n",
      "Epoch 25, Average Loss: 0.1949\n",
      "Training completed!\n",
      "Model saved to models/trained_model.pt\n",
      "Complete model saved to models/model_complete.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training setup\n",
    "loss_fn = nn.MSELoss()  # Regression loss for snap parameter prediction\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in tqdm(range(25)):  # Train for 100 epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for xb, yb in train_loader:\n",
    "        # Initialize hidden state for each batch\n",
    "        ht_1 = torch.zeros(1, xb.size(0), Hout)\n",
    "        \n",
    "        # Forward pass\n",
    "        output, ht = backbone(xb, ht_1)\n",
    "        \n",
    "        # Use the last hidden state for prediction\n",
    "        y_pred = head(ht.squeeze(0))  # Remove batch dimension\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(y_pred, yb)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Create models folder and save the trained model\n",
    "models_dir = Path(\"models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "model_save_path = models_dir / \"trained_model.pt\"\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': avg_loss,\n",
    "    'epoch': 5,\n",
    "    'model_config': {\n",
    "        'Hin': Hin,\n",
    "        'Hout': Hout,\n",
    "        'output_dim': output_dim,\n",
    "        'L': L\n",
    "    }\n",
    "}, model_save_path)\n",
    "\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Also save just the model for easy loading\n",
    "complete_model_path = models_dir / \"model_complete.pt\"\n",
    "torch.save(model, complete_model_path)\n",
    "print(f\"Complete model saved to {complete_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66f26e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete model loaded successfully\n",
      "Loaded checkpoint from epoch 5\n",
      "Final loss: 0.1949\n",
      "Model config: {'Hin': 2, 'Hout': 64, 'output_dim': 5, 'L': 3}\n",
      "Model state dict loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# How to load the saved model from models folder\n",
    "\n",
    "# Method 1: Load complete model (easiest) - with weights_only=False for trusted source\n",
    "loaded_model = torch.load(\"models/model_complete.pt\", weights_only=False)\n",
    "print(\"Complete model loaded successfully\")\n",
    "\n",
    "# Method 2: Load state dict (more flexible)\n",
    "checkpoint = torch.load(\"models/trained_model.pt\")\n",
    "print(f\"Loaded checkpoint from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Final loss: {checkpoint['loss']:.4f}\")\n",
    "print(f\"Model config: {checkpoint['model_config']}\")\n",
    "\n",
    "# Create new model and load state dict\n",
    "new_model = nn.ModuleList([\n",
    "    nn.RNN(input_size=checkpoint['model_config']['Hin'], \n",
    "           hidden_size=checkpoint['model_config']['Hout'], \n",
    "           batch_first=True),\n",
    "    nn.Linear(in_features=checkpoint['model_config']['Hout'], \n",
    "              out_features=checkpoint['model_config']['output_dim'])\n",
    "])\n",
    "\n",
    "new_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Model state dict loaded successfully\")\n",
    "\n",
    "# Example: Make a prediction with loaded model\n",
    "if len(X_data) > 0:\n",
    "    with torch.no_grad():\n",
    "        sample_input = X[:1]  # Take first sample\n",
    "        ht_1 = torch.zeros(1, 1, checkpoint['model_config']['Hout'])\n",
    "        output, ht = new_model[0](sample_input, ht_1)\n",
    "        prediction = new_model[1](ht.squeeze(0))\n",
    "        print(f\"Sample prediction shape: {prediction.shape}\")\n",
    "        print(f\"Sample prediction values: {prediction[0][:5]}\")  # First 5 values\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
